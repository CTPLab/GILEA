import pickle
from pathlib import Path

import clip
import numpy as np
import pyrallis
import torch
import torch.nn.functional as F
from dataclasses import dataclass
from torchvision.transforms import Normalize
from tqdm import tqdm

from configs.paths_config import model_paths
from models.stylegan3.model import SG3Generator


@dataclass
class Options:
    """ Creating delta_i_c file for StyleCLIP's global directions """

    """ StyleGAN Args """
    # Path to StyleGAN model weights
    checkpoint_path: Path = Path(model_paths['stylegan3_ffhq_pt'])
    # Images resolution generated by the StyleGAN model
    stylegan_size: int = 1024
    # Is it the landscape model? If so, different init_kwargs are used to load the pretained StyleGAN model
    is_landscape: bool = False

    """ Precomputed StyleSpace properties """
    # Path to S latent codes precomputed by s_statistics.py
    latents_s_path: Path = Path("stats/S")
    # Path to StyleSpace statistics precomputed by Path to StyleGAN model weights
    latents_statistics_path: Path = Path("stats/s_stats")

    """ Global Direction Args """
    # Manipulation strength used to perturb the latent codes for obtaining directions in CLIP's space. Should be 5 for
    # FFHQ, and 10 for other domains
    manipulation_strength: int = 5

    """ General Args """
    # Path to directory in which result files are saved
    results_path: Path = Path("delta_i_c")
    # Number of images used for computing delta_i_c. We used 300 in our experiments
    num_samples: int = 1


def generate_images(stylegan_model, latents_s, batch_size=1):
    all_images = []
    for i in range(0, latents_s['input'].shape[0], batch_size):
        curr_latents_s = {l: latents_s[l][i:i + batch_size] for l in latents_s}
        with torch.no_grad():
            curr_images = stylegan_model.synthesis(None, all_s=curr_latents_s, noise_mode='const')
        curr_images = F.interpolate(curr_images, size=(224, 224), mode='bicubic', align_corners=True)
        curr_images = (curr_images + 1) / 2
        curr_images = curr_images.clamp(0, 1)
        curr_images = Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))(curr_images)
        all_images.append(curr_images)

    all_images = torch.cat(all_images)
    return all_images


def get_clip_features(clip_model, images):
    images_reshaped = images.view(-1, 3, images.shape[-2], images.shape[-1])
    processed_images = images_reshaped
    with torch.no_grad():
        clip_features = clip_model.encode_image(processed_images)

    clip_features = clip_features.view(images.shape[0], 2, -1).unsqueeze(0)
    return clip_features


def get_delta_i_c(clip_features):
    features_norm = np.linalg.norm(clip_features, axis=-1)
    normalized_features = clip_features / features_norm[:, :, :, None]
    delta_i_c = normalized_features[:, :, 1, :] - normalized_features[:, :, 0, :]
    normalized_delta_i_c = delta_i_c / np.linalg.norm(delta_i_c, axis=-1)[:, :, None]
    normalized_delta_i_c = normalized_delta_i_c.mean(axis=1)
    normalized_delta_i_c = normalized_delta_i_c / np.linalg.norm(normalized_delta_i_c, axis=-1)[:, None]
    return normalized_delta_i_c


@pyrallis.wrap()
def main(args: Options):
    args.results_path.mkdir(exist_ok=True, parents=True)

    G = SG3Generator(args.checkpoint_path, res=args.stylegan_size,
                     config="landscape" if args.is_landscape else None).decoder

    clip_model, clip_preprocess = clip.load("ViT-B/32", device="cuda")

    latents_s = pickle.load(open(str(args.latents_s_path), "rb"))
    latents_s = {l: torch.from_numpy(latents_s[l][:args.num_samples]).float().cuda() for l in latents_s}
    transform, mean, std = pickle.load(open(str(args.latents_statistics_path), "rb"))

    all_clip_features = []
    manipulated_images = torch.zeros(args.num_samples, 2, 3, 224, 224).cuda()

    for layer in latents_s.keys():
        layer_channels = latents_s[layer].shape[1]
        for channel in tqdm(range(layer_channels)):
            for dir_idx, direction in enumerate([-args.manipulation_strength, args.manipulation_strength]):
                latents_s[layer][:, channel] = mean[layer][channel] + direction * std[layer][channel]
                curr_images = generate_images(G, latents_s, batch_size=1)
                manipulated_images[:, dir_idx] = curr_images
            clip_features = get_clip_features(clip_model, manipulated_images)
            all_clip_features.append(clip_features)

    all_clip_features = torch.cat(all_clip_features).detach().cpu().numpy()
    np.save(str(args.results_path / "clip_features.npy"), all_clip_features)

    delta_i_c = get_delta_i_c(all_clip_features)
    np.save(str(args.results_path / "delta_i_c.npy"), delta_i_c)


if __name__ == "__main__":
    main()
